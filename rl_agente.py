import numpy as np # <-- ESSENCIAL!

# ==========================================================
## 1. CRIA√á√ÉO DO AMBIENTE (CyberEnv)
# A classe que define o ambiente de ciberseguran√ßa para o Agente de RL
class CyberEnv: # <-- DEFINI√á√ÉO NECESS√ÅRIA!
    # 0 = Normal, 1 = Baixa Amea√ßa, 2 = M√©dia Amea√ßa, 3 = Alta Amea√ßa
    ESTADOS = [0, 1, 2, 3]
    NUM_ESTADOS = len(ESTADOS)
    
    # 0 = Permitir Conex√£o (PASS), 1 = Bloquear Conex√£o (BLOCK)
    ACOES = [0, 1]
    NUM_ACOES = len(ACOES)

    def __init__(self):
        # O estado inicial ser√° sempre "Normal" (0)
        self.estado_atual = 0

    # M√©todo 1: Gerar um novo estado (simular um novo log de rede)
    def reset(self):
        # Simula a probabilidade de um ataque come√ßar ou escalar.
        self.estado_atual = np.random.choice(self.ESTADOS, p=[0.7, 0.15, 0.1, 0.05])
        return self.estado_atual

    # M√©todo 2: O agente toma uma a√ß√£o e o ambiente calcula a recompensa e o novo estado
    def step(self, acao):
        estado_antes = self.estado_atual
        recompensa = 0
        feito = False # Flag para saber se o epis√≥dio terminou

        # L√≥gica de Recompensa
        if (estado_antes == 3 and acao == 1): # ALTA AMEA√áA + BLOQUEAR
            recompensa = +50
            feito = True
        elif (estado_antes == 0 and acao == 0): # NORMAL + PERMITIR
            recompensa = +1
            
        elif (estado_antes == 3 and acao == 0): # ALTA AMEA√áA + PERMITIR
            recompensa = -100
            feito = True
        elif (estado_antes == 0 and acao == 1): # NORMAL + BLOQUEAR
            recompensa = -5
            
        else:
            recompensa = -1

        # Transi√ß√£o para o Pr√≥ximo Estado
        if estado_antes == 3 and acao == 0:
            proximo_estado = 3 # O ataque persiste/conclui
        elif estado_antes < 3 and acao == 0:
            proximo_estado = estado_antes + 1 # Amea√ßa pode escalar
        else:
            proximo_estado = self.reset() # Amea√ßa foi mitigada ou o ambiente se normaliza
            
        return proximo_estado, recompensa, feito

# ==========================================================
## 2. DESENVOLVIMENTO DO AGENTE (Q-Learning)

# Par√¢metros de Q-Learning
ALFA = 0.1       # Taxa de Aprendizagem (Learning Rate)
GAMA = 0.95       # Fator de Desconto (Discount Factor)
EPSILON = 1.0    # Taxa de Explora√ß√£o (Epsilon)
MIN_EPSILON = 0.01 # M√≠nimo de Explora√ß√£o
DECAY_RATE = 0.001 # Taxa de Decaimento do Epsilon

# 1. Inicializar o ambiente
env = CyberEnv()

# 2. Inicializar a Tabela Q
q_tabela = np.zeros((env.NUM_ESTADOS, env.NUM_ACOES))

# Fun√ß√£o de sele√ß√£o de a√ß√£o (Epsilon-Greedy)
def escolher_acao(estado, epsilon):
    # Explora√ß√£o: com probabilidade epsilon, escolhe uma a√ß√£o aleat√≥ria
    if np.random.random() < epsilon:
        return np.random.randint(env.NUM_ACOES)
    # Explota√ß√£o: escolhe a melhor a√ß√£o baseada na Tabela Q (argmax)
    else:
        return np.argmax(q_tabela[estado, :])

# Fun√ß√£o Principal de Treinamento
def treinar_agente(num_episodios):
    global EPSILON
    historico_recompensas = []
    
    print("Iniciando Treinamento Q-Learning...")

    for episodio in range(num_episodios):
        estado_atual = env.reset() 
        recompensa_total = 0
        feito = False
        
        while not feito:
            # 1. Escolher A√ß√£o
            acao = escolher_acao(estado_atual, EPSILON)
            
            # 2. Executar A√ß√£o e Observar
            proximo_estado, recompensa, feito = env.step(acao)
            
            # 3. Atualizar a Tabela Q (Regra de Bellman)
            q_antigo = q_tabela[estado_atual, acao]
            melhor_q_futuro = np.max(q_tabela[proximo_estado, :])
            
            # F√≥rmula Principal do Q-Learning: Q(s,a) = Q(s,a) + Œ± * [ R + Œ≥ * max(Q(s',a')) - Q(s,a) ]
            novo_q = q_antigo + ALFA * (recompensa + GAMA * melhor_q_futuro - q_antigo)
            q_tabela[estado_atual, acao] = novo_q
            
            estado_atual = proximo_estado
            recompensa_total += recompensa

        # 4. Decaimento do Epsilon
        EPSILON = max(MIN_EPSILON, EPSILON - DECAY_RATE)

        historico_recompensas.append(recompensa_total)
        
        if episodio % 100 == 0:
            print(f"Epis√≥dio {episodio}: Recompensa Total = {recompensa_total:.2f}, Epsilon = {EPSILON:.2f}")

    print("\nTreinamento Conclu√≠do.")
    return historico_recompensas

# ==========================================================
## 3. EXECU√á√ÉO E AN√ÅLISE DE DESEMPENHO

# Executar o treinamento
NUM_EPISODIOS = 1000
historico = treinar_agente(NUM_EPISODIOS)

# 1. Imprimir a Tabela Q Final (O Modelo Aprendido)
print("\n## üìã Tabela Q Final (Conhecimento do Agente) ##")
print("Linhas = Estados (0:Normal a 3:Alta Amea√ßa), Colunas = A√ß√µes (0:Permitir, 1:Bloquear)")
print(q_tabela)

# 2. An√°lise da Pol√≠tica (O que o agente faria em cada Estado)
print("\n## üß† Pol√≠tica Final (Decis√µes Preditivas) ##")
for i in range(env.NUM_ESTADOS):
    acao_otima = np.argmax(q_tabela[i, :])
    decisao = "BLOQUEAR" if acao_otima == 1 else "PERMITIR"
    print(f"Estado de Amea√ßa {i}: Decis√£o √ìtima -> {decisao}")

# 3. An√°lise de Desempenho (M√©dia das recompensas por epis√≥dio)
janela = 50 # Janela de m√©dia m√≥vel para suavizar o gr√°fico
media_movel = np.convolve(historico, np.ones(janela)/janela, mode='valid')

print(f"\nRecompensa M√©dia nos √öltimos {janela} Epis√≥dios: {np.mean(historico[-janela:]):.2f}")

# Exemplo de como voc√™ interpretaria a Tabela Q (Para seu Artigo)
print("\n--- Interpreta√ß√£o para o Artigo ---")
q_alta_ameaca = q_tabela[3, :]
print(f"Q(Estado=3, Permitir): {q_alta_ameaca[0]:.2f}")
print(f"Q(Estado=3, Bloquear): {q_alta_ameaca[1]:.2f}")